# 박스-콕스 변환
preProcValues2 <- preProcess(training, method = "BoxCox")
trainBC <- predict(preProcValues2, training)
testBC <- predict(preProcValues2, test)
preProcValues2
hist(training$AMW)
hist(trainBC$AMW)
hist(training$AMW)
hist(trainBC$AMW)
##### 모형평가
head(iris)
plot(iris[-5],col=iris[[5]])
### data
data.use <- subset(iris, Species == "setosa" | Species == "Virginica")
data.use$Sqecies <- factor(data.use$Species)
str(data.use)
table(data.use$Species)
data.use
##### 모형평가
head(iris)
plot(iris[-5],col=iris[[5]])
### data
data.use <- subset(iris, Species == "setosa" | Species == "Virginica")
data.use$Sqecies <- factor(data.use$Species)
str(data.use)
table(data.use$Species)
iris
### Logistic regression
res.glm <- glm(Species~Sepal.Length, data= data.use, family=binomial)
summary(res.glm)
pred.glm <- predict(res.glm, data.use, type="response")
pred.glm.str <- ifelse(pred.glm<0.5, "setosa","virginica")
pred.glm.factor <- factor(pred.glm.str)
table(data.use$Species, pred.glm.factor)
library(e1071)
res.svm <- svm(Species~Sepal.Length, data=data.use)
library(e1071)
res.svm <- svm(Species~Sepal.Length, data=data.use)
### SVM
install.packages("e1071")
library(e1071)
res.svm <- svm(Species~Sepal.Length, data=data.use)
install.packages("e1071")
library(e1071)
res.svm <- svm(Species~Sepal.Length, data=data.use)
summary(res.svm)
### SVM
install.packages("e1071")
install.packages("e1071")
library(e1071)
res.svm <- svm(Species~Sepal.Length, data=data.use)
library(e1071)
res.svm <- svm(Species~Sepal.Length, data=data.use)
### SVM
install.packages("e1071")
library(e1071)
res.svm <- svm(Species~Sepal.Length, data=data.use)
### SVM
install.packages("e1071")
install.packages("e1071")
library(e1071)
res.svm <- svm(Species~Sepal.Length, data=data.use)
library(e1071)
library(e1071)
# gamma : 선형을 제외한 모든 커널에 요구되는 모수, 기본값은 1/(데이터 차원)
# cost : 제약 위배의 비용, 기본값은 1
# kernel : linear/polynomial/radial/sigmoid
svm.e1071 <- svm(Species ~. , data = iris, type = "C-classification",
kernel = "radial", cost = 10, gamma = 0.1)
summary(svm.e1071)
res.svm <- svm(Species~Sepal.Length, data=data.use)
### data
data.use <- subset(iris, Species == "setosa" || Species == "Virginica")
data.use
iris
### data
data.use <- subset(iris, Species == "setosa" | Species == "Virginica")
data.use
### Data
data.use <- subset(iris, Species=="setosa" | Species=="virginica")
data.use$Species <- factor(data.use$Species)
str(data.use)
table(data.use$Species)
data.use$Species <- factor(data.use$Species)
str(data.use)
table(data.use$Species)
res.svm <- svm(Species~Sepal.Length, data=data.use)
### Logistic regression
res.glm <- glm(Species~Sepal.Length, data= data.use, family=binomial)
summary(res.glm)
pred.glm <- predict(res.glm, data.use, type="response")
pred.glm.str <- ifelse(pred.glm<0.5, "setosa","virginica")
pred.glm.factor <- factor(pred.glm.str)
table(data.use$Species, pred.glm.factor)
data.use$Species
library(pROC)
### SVM
response.value <- as.integer(data.use$Species)-1
predict.value <- as.integer(pred.svm)-1
res.svm <- svm(Species~Sepal.Length, data=data.use)
summary(res.svm)
pred.svm <- predict(res.svm, data.use, type="response")
table(data.use$Species, pred.svm)
### SVM
response.value <- as.integer(data.use$Species)-1
predict.value <- as.integer(pred.svm)-1
roc.svm <- roc(response.value)
### SVM
response.value <- as.integer(data.use$Species)-1
predict.value <- as.integer(pred.svm)-1
roc.svm <- roc(response.value)
response.value
roc.svm <- roc(response.value,predict.value)
roc.svm
### ROC test
roc.test(roc.glm,roc.svm)
### Logistic regression
response.value <- as.integer(data.use$Species)-1
predict.value <- as.integer(pred.glm.factor)-1
roc.glm <- roc(response.value, predict.value)
plot(roc.glm, col="red", print.auc=T)
### SVM
response.value <- as.integer(data.use$Species)-1
predict.value <- as.integer(pred.svm)-1
roc.svm <- roc(response.value, predict.value)
plot(roc.svm, col="blue", print.auc=T, add=T, print.auc.adj=c(1.11,1.2))
### ROC test
roc.test(roc.glm,roc.svm)
a <- subset(iris, Species == "setosa" | Species == "versicolor")
a$Species <- factor(a$Species)
as.numeric(a$Species)
str(a)
b <- glm(Species~Sepal.Length, data=a, family=binomial)
summary(b)
coef(b)
choose.dir()
wd <- "C:/Users/dydeh/4학년2학기/DM"
setwd(wd)
data.file <- "german_05.txt"
data.raw <- read.table(data.file,header=T, encoding = "UTF-8")
data.raw %>% dim()
data.raw %>% head()
library(dplyr)
data.raw %>% dim()
data.raw %>% head()
data.use <- data.raw
####### Out variable
out.var.name <- "class"
out.var.name
data.use[[out.var.name]] <- data.use[[out.var.name]] - 1
data.use %>% head()
data.use[[out.var.name]] %>% table()
####### training / test data 나누기
training.ratio <- 0.5
### training
training.data <- data.use[0,]  ## 구조만 복사하기
training.data
data.size <- data.use %>% nrow()
data.size
training.data <- data.size * training.ratio
training.data
training.data %>% table()
training.index <- sample(1:data.size, data.size/2) ## 1:1000, 500 : 1부터 1000사이의 값 500개
training.index
training.data <- rbind(training.data, data.use[training.index,])
training.data$class %>% table()
###### Over sampling
if(F){
tb <- training.data$class %>% table()
tb
over.n <- tb[1]-tb[2]
training.over.idx <- sample(1:tb[2], over.n, T)
training.data.bad <- training.data %>% filter(class==1)
training.data <- rbind(training.data, training.data.bad[training.over.idx,])
training.data$class %>% table()
}
### test
test.data <- data.use[-training.index,]
test.data$class %>% table()
### Logistic regression
res.glm <- glm(class~., data=data.use, family=binomial) # class~. : class변수만 제외하고 모두 다
summary(res.glm)
res.glm.step <- step(res.glm)  #aic 필요없는 변수 제거하기
summary(res.glm.step)
pred.glm <- predict(res.glm.step, test.data, type="response")  # 0~1사이의 값.
pred.glm.class <- ifelse(pred.glm<0.05,0,1)
### 정확도, 민감도, 특이도
###### cross table
tb<-table(test.data$class, pred.glm.class)
tb
tptn <- sum(diag(tb)) # 제대로 예측한 값
n <- sum(tb) # 테이블값을 모두 더한 값
tptn/n # 정분류율(정확도)
tb[1,1]/(tb[1,1]+tb[1,2]) # 민감도
tb[2,2]/(tb[2,1]+tb[2,2]) # 특이도
##### 로지스틱 회귀 모형
data(iris)
a <- subset(iris, Species == "setosa" | Species == "versicolor")
a$Species <- factor(a$Species)
as.numeric(a$Species)
str(a)
b <- glm(Species~Sepal.Length, data=a, family=binomial)
summary(b)
coef(b)
exp(coef(b))
#신뢰구간 확인하기
confint.sl <- confint(b, parm = "Sepal.Length")
exp(confint.sl)
fitted(b)
predict(b, newdata=a[c(1,50,51,100),],type = "response")
predict(b, newdata=a[1:5,], type = "response")
predict(b, newdata=a[95:100,], type = "response")
# cdplot() 함수는 번주형 변수의 조건부분포를 보여줌
cdplot(Species~Sepal.Length, data=a)
plot(a$Sepal.Length, a$Species, xlab="Sepal.Length")
x=seq(min(a$Sepal.Length), max(a$Sepal.Length),0.1)
lines(x, 1+(1/(1+(1/exp(-27.831+5.140*x)))),type="l",col="red")
exp(confint.sl)
#신뢰구간 확인하기
confint.sl <- confint(b, parm = "Sepal.Length")
# cdplot() 함수는 번주형 변수의 조건부분포를 보여줌
cdplot(Species~Sepal.Length, data=a)
summary(b)
coef(b)
str(a)
as.numeric(a$Species)
str(a)
summary(b)
coef(b)
exp(coef(b))
#신뢰구간 확인하기
confint.sl <- confint(b, parm = "Sepal.Length")
exp(confint.sl)
fitted(b)
predict(b, newdata=a[c(1,50,51,100),],type = "response")
predict(b, newdata=a[1:5,], type = "response")
predict(b, newdata=a[95:100,], type = "response")
# cdplot() 함수는 번주형 변수의 조건부분포를 보여줌
cdplot(Species~Sepal.Length, data=a)
plot(a$Sepal.Length, a$Species, xlab="Sepal.Length")
# cdplot() 함수는 번주형 변수의 조건부분포를 보여줌
cdplot(Species~Sepal.Length, data=a)
plot(a$Sepal.Length, a$Species, xlab="Sepal.Length")
glm.vs <- glm(vs~mpg+am, data = mtcars, family=binomial)
summary(glm.vs)
# direction = c("both", "backward", "forward")
# both : 전진후진 반복, backward : 후진, forwad : 전진
step.vs <- step(glm.vs, direction="backward")
summary(step.vs)
ls(glm.vs)
str(glm.vs)
anova(glm.vs, test="Chisq")
pred.glm <- predict(res.glm.step, test.data, type="response")  # 0~1사이의 값.
pred.glm.class <- ifelse(pred.glm<0.05,0,1)
pred.glm.class
data.raw %>% head()
data.use[[out.var.name]] %>% table()
pred.glm.class
training.data %>% table()
data.file <- "german_05.txt"
data.raw <- read.table(data.file,header=T, encoding = "UTF-8")
data.use[[out.var.name]] <- data.use[[out.var.name]] - 1
source('C:/Users/dydeh/4학년2학기/DM/DM_로지스틱회귀 실습.R', encoding = 'UTF-8', echo=TRUE)
test.data$class %>% table()
training.data$class %>% table()
### test
test.data <- data.use[-training.index,]
test.data
test.data$class %>% table()
training.data$class %>% table()
tb
c <- rpart(Species~., data=iris)
library(rpart)
head(iris)
c <- rpart(Species~., data=iris)
plot(c, compress=T, margin=0.3)
text(c, cex=1.2)
text(c, cex=2.2)
plot(c, compress=T, margin=0.3)
plot(c, compress=T, margin=0.6)
plot(c, compress=T, margin=0.1)
text(c, cex=1.2)
plot(c, compress=T, margin=0.4)
text(c, cex=1.2)
plot(c, compress=T, margin=0.7)
text(c, cex=1.2)
plot(c, compress=T, margin=0.3)
text(c, cex=1.2)
c.pred <- predict(c, newdata=iris, type = "class")
table(iris$Species, c.pred)
c.pred
library(rpart.plot)
prp(c, type=4, extra = 2)
install.packages("rpart.plot")
library(rpart.plot)
prp(c, type=4, extra = 2)
ls(c)
set.seed(1234)
ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7,0.3))
stagec3 <- na.omit(stagec) # na값을 모두 지워준다.
stagec3 %>% dim()
set.seed(1234)
ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7,0.3))
ind
table(ind)
trainData <- stagec3[ind==1,]
testData <- stagec3[ind==2,]
trainData %>% dim()
testData %>% dim()
tree <- ctree(ploidy~., data=trainData) # trainData를 이용해서 ploidy를 제외한 7개의 변수를 사용
tree
tree <- ctree(ploidy~., data=trainData) # trainData를 이용해서 ploidy를 제외한 7개의 변수를 사용
library(party)
install.packages("party")
library(party)
tree <- ctree(ploidy~., data=trainData) # trainData를 이용해서 ploidy를 제외한 7개의 변수를 사용
tree
plot(tree)
testPred <- predict(tree, newdata=testData)
testPred
# 예측을 잘 했는지 교차표만들어보기
table(testData$ploidy, testPred)
# gamma : 선형을 제외한 모든 커널에 요구되는 모수, 기본값은 1/(데이터 차원)
# cost : 제약 위배의 비용, 기본값은 1
# kernel : linear/polynomial/radial/sigmoid
svm.e1071 <- svm(Species ~. , data = iris, type = "C-classification",
kernel = "radial", cost = 10, gamma = 0.1)
?svm
summary(svm.e1071)
plot(svm.e1071, iris, Petal.Width ~ Petal.Length, slice = list(Sepal.Width = 3, Sepal.Length = 4))
pred <- predict(svm.e1071, iris, decision.values = T)
plot(svm.e1071, iris, Sepal.Width ~ Sepal.Length, slice = list(Petal.Width = 2.5, Petal.Length = 3))
(acc <- table(pred, iris$Species))
pred <- predict(svm.e1071, iris, decision.values = T)
pred
(acc <- table(pred, iris$Species))
data(iris)
x <- c(1:20)
y <- c(3,4,8,4,6,9,8,12,15,26,35,40,45,54,49,59,60,62,63,68)
data <- data.frame(x,y)
plot(data, pch=16)
## 회귀모형
model <- lm(y ~ x, data)
model %>% summary()
abline(model, col = 'red')
model %>% mode()
model %>% names()
#lm.error <- model$residuals
#(lmRMSE <- sqrt(mean(lm.error^2)))
lmRMSE <- model$residuals^2 %>% mean() %>% sqrt()
## SVM
model <- svm(y ~ x, data=data)
model %>% mode()
model %>% names()
svmRMSE <- model$residuals^2 %>% mean() %>% sqrt()
svmRMSE
pred.svm <- predict(model, data)
plot(x,pred.svm, type="o")
lines(x,y,type="o", col="red") # 이전의 선 추가로 긋기
plot(x,pred.svm, type="o")
plot(data, pch=16)
plot(data, pch=10)
plot(data, pch=12)
plot(data, pch=13)
plot(data, pch=1)
lines(x,y,type="o", col="red") # 이전의 선 추가로 긋기
plot(x,pred.svm, type="o")
lines(x,y,type="o", col="red") # 이전의 선 추가로 긋기
plot(data, pch=1)
lines(x,y,type="o", col="red") # 이전의 선 추가로 긋기
plot(x,pred.svm, type="o")
lines(x,y,type="o", col="red") # 이전의 선 추가로 긋기
plot(data, pch=16)
library(nnet)
head(iris)
# size : 노드 개수(hidden layer/층수), maxit : 반복횟수, decay : 5e-04가 디폴트, rang : [-rang:rang] 무작위 가중
nn.iris <- nnet(Species~., data=iris, size=2, rang=0.1, decay=5e-4, maxit=200)
nn.iris  %>% summary()
nn.iris  %>% summary() %>% plot()
nn.iris  %>% summary()
library(devtools)
install.packages("devtools")
library(devtools)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
plot.nnet(nn.iris)
# size : 노드 개수(hidden layer/층수), maxit : 반복횟수, decay : 5e-04가 디폴트, rang : [-rang:rang] 무작위 가중
nn.iris <- nnet(Species~., data=iris, size=2, rang=0.1, decay=5e-4, maxit=200)
nn.iris  %>% summary()
plot.nnet(nn.iris)
library(devtools)
plot.nnet(nn.iris)
plot.nnet
?plot.nnet
??plot.nnet
plot.nnet(nn.iris)
plot.net(nn.iris)
pred.iris <- predict(nn.iris, iris, type="class")
table(iris$Species, pred.iris)
### <ex2>
data(infert, package="datasets")
str(infert)
infert %>% dim()
infert %>% head()
library(neuralnet)
net.infert <- neuralnet(case~age+parity+induced+spontaneous, data=infert, hidden=2, err.fct="ce",
linear.output=FALSE, likelihood=TRUE)
net.infert
mode(net.infert)
plot(net.infert)
names(net.infert)
net.infert$result.matrix
# 적합값은 $net.result에 제공
out <- cbind(net.infert$covariate, net.infert$net.result[[1]])
dimnames(out) <- list(NULL, c("age","parity", "induced", "spontaneous", "nn-output"))
head(out)
# 가중치의 초깃값과 적합값은 $startweights와 $weights에 제공
# 일반화 가중치
head(net.infert$generalized.weights[[1]])
# 일반화 가중치 시각화
par(mfrow=c(2,2))
gwplot(net.infert, selected.covariate = "age", min=-2.5, max=5)
gwplot(net.infert, selected.covariate = "parity", min=-2.5, max=5)
gwplot(net.infert, selected.covariate = "induced", min=-2.5, max=5)
gwplot(net.infert, selected.covariate = "spontaneous", min=-2.5, max=5)
par(mfrow=c(1,1))
new.output <- compute(net.infert, covariate=matrix(c(22,1,0,0,
22,1,1,0,
22,1,0,1,
22,1,1,1),
byrow=TRUE, ncol=4))
# 공변량 조합에 대한 예측결과, 사전 낙태의 수에 따라 예측확률이 증가함을 보여줌
new.output$net.result
# 위 인공신경망 모델에서 각 변수의 중요도 확인
library("NeuralNetTools")
library("curl")
library("ROCR")
data.file <- "https://raw.githubusercontent.com/adriangasinski/datahacking_0001/master/german_credit.csv"
# stringsAsFactors : 문자열 그대로 읽어오기
data.raw <- fread(data.file, stringsAsFactors=T)
write.csv(data.raw, "german_credit.csv", row.names=F)
data.raw %>% dim()
data.raw %>% head()
data.raw %>% summary()
data.raw %>% names()
data.raw %>% select(default) %>% class()
data.raw %>% select(default) %>% table()
data.raw %>% sapply(class)
### 숫자로 변환
data.use <- data.raw %>% sapply(as.numeric)
data.use %>% head()
### 데이터 분할
inTrain <- createDataPartition(y=data.use[,"default"], p=0.6, list=FALSE)  # 학습,테스트데이터 나누기
data.train <- data.use[inTrain,]
data.test <- data.use[-inTrain,]
### 볌위 [0, 1] 조정
data.train.pp <- data.train %>% preProcess(method="range")
data.train <- predict(data.train.pp, data.train)
data.test <- predict(data.train.pp, data.test)
### 모델링
nnet.model.1 <- neuralnet(default~., data=data.train, hidden=5, threshold=0.01)
nnet.model.2 <- neuralnet(default~., data=data.train, hidden=c(2,2),  threshold=0.01)
### 신경망 구조 다이어그램: 해석하지 않음
plot(nnet.model.1)
### 변수 중요도
garson(nnet.model.1) + coord_flip()
# 계층분석
library(dplyr)
library(dyplyr)
library(dplyr)
install.packages("rattle")
library(rattle)
data(wine)
# K-means에는 여러가지 라이브러리가 있다.
install.packages("NbClust")
library(NbClust)
wine %>% head()
wine %>% summary()
df <- scale(wine[1]) # 첫 번째 데이터 제거
df <- scale(wine[-1]) # 첫 번째 데이터 제거
df
# nc : number of clust
nc <- NbClust(df, min.nc = 2, max.nc = 15, method = "kmeans")
set.seed(1234)
table(nc$Best.nc)
table(nc$Best.nc[1,])
fit.km <- kmeans(df, 3, nstatrt=25) # nstart : 다중의 초깃값 수
fit.km <- kmeans(df, 3, nstart=25) # nstart : 다중의 초깃값 수
fit.km
fit.km$size
plot(df, col=fit.kkm$sluster)
plot(df, col=fit.km$sluster)
point(fit.km$centers, col=1:3, pch=8, cex=1.5) # pch : 점종류
points(fit.km$centers, col=1:3, pch=8, cex=1.5) # pch : 점종류
plot(df, col=fit.km$sluster)
points(fit.km$centers, col=1:3, pch=8, cex=1.5) # pch : 점종류, cex : 점크기
fit.km$centers
plot(df, col=fit.km$cluster)
par(mfrow=c(1,1))
plot(df, col=fit.km$cluster)
points(fit.km$centers, col=1:3, pch=8, cex=1.5) # pch : 점종류, cex : 점크기
# 각 군집별로 변수의 요약값을 측정단위의 척도로 표시
aggregate(wine[-1], by=list(cluster=fit.km$cluster), mean)
# 정오분류표
ct.km <- table(wine$Type, fit.km$cluster)
ct.km
fit.km
ct.km
# 군집 간의 일지도(agreement)
install.packages("flexclust")
library(flexclust)
randIndex(ct.km)
